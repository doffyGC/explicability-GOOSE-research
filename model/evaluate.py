import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score
from scipy.stats import sem, t
from datetime import datetime
import os
from config import MODEL_TYPE
import time
import pickle
import tracemalloc
import sys
import tracemalloc
import sys

def mean_confidence_interval(data, confidence=0.95):
    """
    Calcula a mÃ©dia e o intervalo de confianÃ§a de um conjunto de dados.

    Args:
        data (list/array): Dados pra calcular a mÃ©dia e IC.
        confidence (float): NÃ­vel de confianÃ§a (padrÃ£o: 0.95 = 95%).

    Returns:
        tuple: (mean, margin_of_error)
            - mean: MÃ©dia dos dados
            - margin_of_error: Margem de erro (metade do IC)
    """
    n = len(data)
    m = np.mean(data)
    se = sem(data)  # Erro padrÃ£o da mÃ©dia

    # Tratamento especial: se todos os valores sÃ£o idÃªnticos (sem = 0 ou nan)
    # isso acontece quando o modelo tem desempenho perfeito em todos os folds
    if np.isnan(se) or se == 0:
        return m, 0.0

    h = se * t.ppf((1 + confidence) / 2., n-1)  # Margem de erro usando t de Student
    return m, h


def evaluate_models(cv_models, final_model, X_test, y_test, class_names):
    """
    Avalia mÃºltiplos modelos da validaÃ§Ã£o cruzada E o modelo final no teste hold-out.

    IMPORTANTE: Segue boas prÃ¡ticas acadÃªmicas!
    1. Calcula mÃ©tricas de VALIDAÃ‡ÃƒO CRUZADA (mean Â± CI) pros folds
    2. Calcula mÃ©tricas do TESTE FINAL no conjunto hold-out
    3. Agrega Cohen's Kappa corretamente (mean Â± CI, nÃ£o lista!)

    Args:
        cv_models (list): Lista de tuplas (model, X_val, y_val) dos folds de CV.
        final_model: Modelo final treinado em todos os dados de treino.
        X_test (pd.DataFrame): Features do conjunto de teste final (hold-out).
        y_test (np.array): Classes do conjunto de teste final (hold-out).
        class_names (list): Lista com os nomes das classes.

    Returns:
        tuple: (cv_metrics_summary, test_metrics, kappa_mean, kappa_ci)
            - cv_metrics_summary: MÃ©tricas da validaÃ§Ã£o cruzada (mean Â± CI por classe)
            - test_metrics: MÃ©tricas do teste final (hold-out)
            - kappa_mean: MÃ©dia do Cohen's Kappa nos folds de CV
            - kappa_ci: Intervalo de confianÃ§a do Cohen's Kappa
            - test_kappa: Kappa no teste final
            - test_cm: Matriz de confusÃ£o do teste final
            - cv_total_cm: Matriz de confusÃ£o agregada da CV
            - deployment_metrics: MÃ©tricas de latÃªncia/memÃ³ria para inferÃªncia
    """

    print("=" * 60)
    print("AVALIAÃ‡ÃƒO: MÃ©tricas da ValidaÃ§Ã£o Cruzada")
    print("=" * 60)

    # ========================================
    # PARTE 1: MÃ©tricas da ValidaÃ§Ã£o Cruzada
    # ========================================

    # Coleta as mÃ©tricas de cada fold
    accuracy_scores = []
    kappa_scores = []
    precision_scores = {cls: [] for cls in class_names}
    recall_scores = {cls: [] for cls in class_names}
    f1_scores = {cls: [] for cls in class_names}
    # Matriz de confusÃ£o agregada (soma das matrizes dos folds)
    n_classes = len(class_names)
    cv_total_cm = np.zeros((n_classes, n_classes), dtype=int)

    for fold_num, (model, X_val, y_val) in enumerate(cv_models, 1):
        # Prediz no conjunto de validaÃ§Ã£o desse fold
        y_pred = model.predict(X_val)

        # Calcula report com precision, recall e f1 por classe
        report = classification_report(y_val, y_pred, target_names=class_names, output_dict=True)

        # Calcula Cohen's Kappa (mede concordÃ¢ncia alÃ©m do acaso)
        kappa = cohen_kappa_score(y_val, y_pred)
        kappa_scores.append(kappa)

        # Calcula acurÃ¡cia por classe (diagonal da matriz de confusÃ£o)
        cm = confusion_matrix(y_val, y_pred)
        # Agrega a matriz de confusÃ£o deste fold na matriz total
        try:
            cv_total_cm += cm
        except Exception:
            # Caso haja problemas de shape, assegura conversÃ£o para array
            cv_total_cm = cv_total_cm + np.asarray(cm, dtype=int)
        class_acc = cm.diagonal() / cm.sum(axis=1)
        accuracy_scores.append(class_acc)

        # Coleta precision, recall e f1 de cada classe
        for cls in class_names:
            precision_scores[cls].append(report[cls]['precision'])
            recall_scores[cls].append(report[cls]['recall'])
            f1_scores[cls].append(report[cls]['f1-score'])

        print(f"Fold {fold_num}: Kappa = {kappa:.4f}")

    # Agrega os resultados (mean Â± CI)
    accuracy_scores = np.array(accuracy_scores)
    accuracy_conf_intervals = [mean_confidence_interval(accuracy_scores[:, i]) for i in range(len(class_names))]

    # CORRIGIDO: Agrega o Kappa corretamente (era uma lista, agora Ã© mean Â± CI)
    kappa_mean, kappa_ci = mean_confidence_interval(kappa_scores)
    print(f"\nâœ“ Cohen's Kappa (CV): {kappa_mean:.4f} Â± {kappa_ci:.4f}")
    print()

    # Mostra a matriz de confusÃ£o agregada dos folds
    print("Matriz de ConfusÃ£o Agregada (CV - soma de todos os folds):")
    print(cv_total_cm)
    print()

    # Monta o resumo das mÃ©tricas por classe
    cv_metrics_summary = {
        "Classe": [],
        "Precision Mean": [], "Precision CI": [],
        "Recall Mean": [], "Recall CI": [],
        "F1-score Mean": [], "F1-score CI": [],
        "Accuracy Mean": [], "Accuracy CI": []
    }

    for i, cls in enumerate(class_names):
        pm, pci = mean_confidence_interval(precision_scores[cls])
        rm, rci = mean_confidence_interval(recall_scores[cls])
        f1m, f1ci = mean_confidence_interval(f1_scores[cls])
        acc_m, acc_ci = accuracy_conf_intervals[i]

        cv_metrics_summary["Classe"].append(cls)
        cv_metrics_summary["Precision Mean"].append(pm)
        cv_metrics_summary["Precision CI"].append(pci)
        cv_metrics_summary["Recall Mean"].append(rm)
        cv_metrics_summary["Recall CI"].append(rci)
        cv_metrics_summary["F1-score Mean"].append(f1m)
        cv_metrics_summary["F1-score CI"].append(f1ci)
        cv_metrics_summary["Accuracy Mean"].append(acc_m)
        cv_metrics_summary["Accuracy CI"].append(acc_ci)

    # ========================================
    # PARTE 2: MÃ©tricas do Teste Final (Hold-out)
    # ========================================

    print("=" * 60)
    print("AVALIAÃ‡ÃƒO: MÃ©tricas do Teste Final (Hold-out)")
    print("=" * 60)

    # Avalia o modelo final no conjunto de teste que NUNCA foi visto
    y_test_pred = final_model.predict(X_test)

    # Report completo
    test_report = classification_report(y_test, y_test_pred, target_names=class_names, output_dict=True)

    # Cohen's Kappa do teste final
    test_kappa = cohen_kappa_score(y_test, y_test_pred)

    # Matriz de confusÃ£o
    test_cm = confusion_matrix(y_test, y_test_pred)
    test_class_acc = test_cm.diagonal() / test_cm.sum(axis=1)

    # Monta dicionÃ¡rio com mÃ©tricas do teste
    test_metrics = {
        "Classe": [],
        "Precision": [],
        "Recall": [],
        "F1-score": [],
        "Accuracy": []
    }

    for i, cls in enumerate(class_names):
        test_metrics["Classe"].append(cls)
        test_metrics["Precision"].append(test_report[cls]['precision'])
        test_metrics["Recall"].append(test_report[cls]['recall'])
        test_metrics["F1-score"].append(test_report[cls]['f1-score'])
        test_metrics["Accuracy"].append(test_class_acc[i])

    print(f"âœ“ Cohen's Kappa (Teste): {test_kappa:.4f}")
    print(f"âœ“ AcurÃ¡cia Global (Teste): {test_report['accuracy']:.4f}")
    print()

    # Mostra matriz de confusÃ£o
    print("Matriz de ConfusÃ£o (Teste Final):")
    print(test_cm)
    print()

    # ========================================
    # PARTE 3: MÃ©tricas de Desempenho de InferÃªncia
    # ========================================

    print("=" * 60)
    print("AVALIAÃ‡ÃƒO: MÃ©tricas de Desempenho de InferÃªncia")
    print("=" * 60)

    # Garantir numpy array para fatias de lote
    X_test_np = X_test.values if hasattr(X_test, 'values') else np.asarray(X_test)

    batch_sizes = [1, 8, 32, 128]
    repetitions = 10  # Aumentado para melhor estatÃ­stica

    # MediÃ§Ã£o de latÃªncia por batch size
    latency_results = {}
    all_times_for_analysis = []  # Para estatÃ­sticas globais
    
    for bs in batch_sizes:
        # Limitar ao tamanho disponÃ­vel
        n = min(len(X_test_np), bs)
        if n == 0:
            continue
        x_batch = X_test_np[:n]
        times = []
        for _ in range(repetitions):
            start = time.perf_counter()
            _ = final_model.predict(x_batch)
            end = time.perf_counter()
            elapsed = end - start
            times.append(elapsed)
            all_times_for_analysis.append(elapsed)
        
        times_ms = np.array(times) * 1000  # Converter para ms
        mean_time = np.mean(times)
        
        latency_results[bs] = {
            "mean_ms": float(np.mean(times_ms)),
            "std_ms": float(np.std(times_ms, ddof=1)) if len(times_ms) > 1 else 0.0,
            "p95_ms": float(np.percentile(times_ms, 95)),
            "p99_ms": float(np.percentile(times_ms, 99)),
            "max_ms": float(np.max(times_ms)),
            "per_sample_us": float((mean_time / n) * 1e6),
            "throughput_samples_per_sec": float(n / mean_time) if mean_time > 0 else 0.0
        }

    # Processing Time per Sample (mÃ©dia global de batch size 1)
    processing_time_per_sample_us = latency_results.get(1, {}).get("per_sample_us", 0.0)

    # Tamanho do modelo em memÃ³ria (serializado)
    try:
        model_bytes = pickle.dumps(final_model)
        model_size_mb = len(model_bytes) / (1024 * 1024)
    except Exception:
        model_size_mb = None

    # Runtime memory (memÃ³ria usada durante inferÃªncia)
    tracemalloc.start()
    baseline_memory = tracemalloc.get_traced_memory()[0]
    
    # Fazer algumas prediÃ§Ãµes para capturar uso de memÃ³ria
    test_batch = X_test_np[:min(100, len(X_test_np))]
    for _ in range(5):
        _ = final_model.predict(test_batch)
    
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    runtime_memory_mb = (peak - baseline_memory) / (1024 * 1024)
    memory_overhead_mb = runtime_memory_mb

    # GOOSE deadline compatibility (IEC 61850: tÃ­pico 3-4ms)
    goose_deadline_ms = 3.0  # Threshold padrÃ£o do protocolo GOOSE
    single_sample_latency_ms = latency_results.get(1, {}).get("mean_ms", 0.0)
    goose_compatible = single_sample_latency_ms <= goose_deadline_ms

    # Real-time capability (mensagens por segundo)
    real_time_msg_per_sec = latency_results.get(1, {}).get("throughput_samples_per_sec", 0.0)

    # Memory feasibility (comparaÃ§Ã£o com limites tÃ­picos de edge devices)
    memory_limits = {
        "minimal": 256,  # 256 MB
        "standard": 512,  # 512 MB
        "comfortable": 1024  # 1 GB
    }
    
    total_memory_estimate_mb = (model_size_mb or 0) + runtime_memory_mb
    memory_feasibility = "Unknown"
    if total_memory_estimate_mb <= memory_limits["minimal"]:
        memory_feasibility = "Minimal (â‰¤256MB) âœ“"
    elif total_memory_estimate_mb <= memory_limits["standard"]:
        memory_feasibility = "Standard (â‰¤512MB) âœ“"
    elif total_memory_estimate_mb <= memory_limits["comfortable"]:
        memory_feasibility = "Comfortable (â‰¤1GB) âœ“"
    else:
        memory_feasibility = "High Memory (>1GB) âš "

    # Latency scaling (comparaÃ§Ã£o entre batch sizes)
    latency_scaling = {}
    if 1 in latency_results and 128 in latency_results:
        time_1 = latency_results[1]["mean_ms"]
        time_128 = latency_results[128]["mean_ms"]
        scaling_factor = time_128 / (time_1 * 128) if time_1 > 0 else 0
        latency_scaling = {
            "batch_1_to_128_efficiency": float(scaling_factor),
            "interpretation": "Eficiente" if scaling_factor < 1.2 else "Ineficiente" if scaling_factor > 2.0 else "Moderado"
        }

    deployment_metrics = {
        "latency": latency_results,
        "processing_time_per_sample_us": processing_time_per_sample_us,
        "model_size_mb": model_size_mb,
        "runtime_memory_mb": runtime_memory_mb,
        "memory_overhead_mb": memory_overhead_mb,
        "total_memory_estimate_mb": total_memory_estimate_mb,
        "memory_feasibility": memory_feasibility,
        "goose_deadline_ms": goose_deadline_ms,
        "goose_compatible": goose_compatible,
        "real_time_capability_msg_per_sec": real_time_msg_per_sec,
        "latency_scaling": latency_scaling
    }

    # Print resumo expandido
    print("\nðŸ“Š LATÃŠNCIA:")
    for bs in sorted(latency_results.keys()):
        lr = latency_results[bs]
        print(f"  Batch {bs:3d}: Mean={lr['mean_ms']:7.3f}ms | Std={lr['std_ms']:6.3f}ms | "
              f"P95={lr['p95_ms']:7.3f}ms | P99={lr['p99_ms']:7.3f}ms | Max={lr['max_ms']:7.3f}ms")
        print(f"             Per-sample={lr['per_sample_us']:8.2f}Âµs | Throughput={lr['throughput_samples_per_sec']:8.1f} samples/s")
    
    print(f"\nâœ“ Processing Time per Sample: {processing_time_per_sample_us:.2f} Âµs")
    
    print("\nðŸ’¾ MEMÃ“RIA:")
    if model_size_mb is not None:
        print(f"  Model size (serialized):  {model_size_mb:8.2f} MB")
    print(f"  Runtime memory (peak):    {runtime_memory_mb:8.2f} MB")
    print(f"  Memory overhead:          {memory_overhead_mb:8.2f} MB")
    print(f"  Total estimate:           {total_memory_estimate_mb:8.2f} MB")
    print(f"  Memory feasibility:       {memory_feasibility}")
    
    print("\nâš¡ DEPLOYMENT / REAL-TIME:")
    print(f"  GOOSE deadline (3ms):     {'âœ“ COMPATIBLE' if goose_compatible else 'âœ— NOT COMPATIBLE'} ({single_sample_latency_ms:.3f}ms)")
    print(f"  Real-time capability:     {real_time_msg_per_sec:.1f} msg/s")
    
    if latency_scaling:
        print(f"  Latency scaling (1â†’128):  {latency_scaling['interpretation']} (factor: {latency_scaling['batch_1_to_128_efficiency']:.2f})")
    
    print("\nðŸ“Œ NOTAS:")
    print("  - Overhead in passive monitoring: LatÃªncia adicional depende de integraÃ§Ã£o com sistema")
    print("  - Embedded device performance: Valores sÃ£o estimativas; teste em hardware real recomendado")
    print("  - Efficiency trade-offs: Considere quantizaÃ§Ã£o para reduzir model size e latÃªncia")
    print()

    return cv_metrics_summary, test_metrics, kappa_mean, kappa_ci, test_kappa, test_cm, cv_total_cm, deployment_metrics


def save_metrics_report(cv_metrics, test_metrics, kappa_mean, kappa_ci, test_kappa,
                       test_cm, class_names, dataset_name, output_dir="./results", cv_total_cm=None, deployment_metrics=None):
    """
    Salva um relatÃ³rio completo das mÃ©tricas em formato Markdown e Log.

    Args:
        cv_metrics (dict): MÃ©tricas da validaÃ§Ã£o cruzada.
        test_metrics (dict): MÃ©tricas do teste final.
        kappa_mean (float): MÃ©dia do Kappa na CV.
        kappa_ci (float): IC do Kappa na CV.
        test_kappa (float): Kappa do teste final.
        test_cm (np.array): Matriz de confusÃ£o do teste final.
        cv_total_cm (np.array, optional): Matriz de confusÃ£o agregada da CV (soma dos folds).
        class_names (list): Nomes das classes.
        dataset_name (str): Nome do dataset.
        output_dir (str): DiretÃ³rio onde salvar os relatÃ³rios.

    Returns:
        tuple: (caminho_markdown, caminho_log)
    """

    # Cria o diretÃ³rio se nÃ£o existir
    os.makedirs(output_dir, exist_ok=True)

    # Timestamp pra nome do arquivo
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Nomes dos arquivos
    md_filename = f"metrics_report_{dataset_name}_{timestamp}.md"
    log_filename = f"metrics_report_{dataset_name}_{timestamp}.log"

    md_path = os.path.join(output_dir, md_filename)
    log_path = os.path.join(output_dir, log_filename)

    # ========================================
    # GERA RELATÃ“RIO EM MARKDOWN
    # ========================================

    md_content = f"""# RelatÃ³rio de MÃ©tricas - {MODEL_TYPE.upper()}

**Dataset:** {dataset_name}
**Data/Hora:** {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}

---

## ðŸ“Š ValidaÃ§Ã£o Cruzada (K-Fold)

Resultados da validaÃ§Ã£o cruzada com **intervalo de confianÃ§a de 95%** (IC 95%).

### MÃ©tricas por Classe

"""

    # Tabela de mÃ©tricas da CV
    md_content += "| Classe | F1-Score | Precision | Recall | Accuracy |\n"
    md_content += "|--------|----------|-----------|--------|----------|\n"

    for i, cls in enumerate(class_names):
        f1_mean = cv_metrics['F1-score Mean'][i]
        f1_ci = cv_metrics['F1-score CI'][i]
        prec_mean = cv_metrics['Precision Mean'][i]
        prec_ci = cv_metrics['Precision CI'][i]
        rec_mean = cv_metrics['Recall Mean'][i]
        rec_ci = cv_metrics['Recall CI'][i]
        acc_mean = cv_metrics['Accuracy Mean'][i]
        acc_ci = cv_metrics['Accuracy CI'][i]

        md_content += f"| **{cls}** | {f1_mean:.4f} Â± {f1_ci:.4f} | {prec_mean:.4f} Â± {prec_ci:.4f} | {rec_mean:.4f} Â± {rec_ci:.4f} | {acc_mean:.4f} Â± {acc_ci:.4f} |\n"

    md_content += f"\n### MÃ©tricas Globais (CV)\n\n"
    md_content += f"- **Cohen's Kappa:** {kappa_mean:.4f} Â± {kappa_ci:.4f}\n"
    md_content += f"\n---\n\n"

    # Se disponÃ­vel, inclui a matriz de confusÃ£o agregada da CV
    if cv_total_cm is not None:
        md_content += "## ðŸ§¾ Matriz de ConfusÃ£o (CV - Agregada)\n\n"
        md_content += "```\nPredito â†’    "
        for cls in class_names:
            md_content += f"{cls:>12} "
        md_content += "\nReal â†“\n"

        for i, cls in enumerate(class_names):
            md_content += f"{cls:12} "
            for j in range(len(class_names)):
                md_content += f"{cv_total_cm[i][j]:>12} "
            md_content += "\n"

        md_content += "```\n\n"

    # ========================================
    # MÃ©tricas do Teste Final
    # ========================================

    md_content += f"## ðŸŽ¯ Teste Final (Hold-out)\n\n"
    md_content += f"Resultados no conjunto de teste final (nunca visto durante o treinamento).\n\n"
    md_content += f"### MÃ©tricas por Classe\n\n"

    # Tabela de mÃ©tricas do teste
    md_content += "| Classe | F1-Score | Precision | Recall | Accuracy |\n"
    md_content += "|--------|----------|-----------|--------|----------|\n"

    for i, cls in enumerate(class_names):
        f1 = test_metrics['F1-score'][i]
        prec = test_metrics['Precision'][i]
        rec = test_metrics['Recall'][i]
        acc = test_metrics['Accuracy'][i]

        md_content += f"| **{cls}** | {f1:.4f} | {prec:.4f} | {rec:.4f} | {acc:.4f} |\n"

    md_content += f"\n### MÃ©tricas Globais (Teste)\n\n"
    md_content += f"- **Cohen's Kappa:** {test_kappa:.4f}\n\n"

    # Matriz de confusÃ£o
    md_content += f"### Matriz de ConfusÃ£o\n\n"
    md_content += "```\n"

    # Header
    md_content += "Predito â†’    "
    for cls in class_names:
        md_content += f"{cls:>12} "
    md_content += "\n"
    md_content += "Real â†“\n"

    # Linhas da matriz
    for i, cls in enumerate(class_names):
        md_content += f"{cls:12} "
        for j in range(len(class_names)):
            md_content += f"{test_cm[i][j]:>12} "
        md_content += "\n"

    md_content += "```\n\n"

    # ========================================
    # Desempenho de InferÃªncia
    # ========================================
    if deployment_metrics is not None:
        md_content += f"## âš™ï¸ Desempenho de InferÃªncia\n\n"
        
        # SeÃ§Ã£o de LatÃªncia
        md_content += f"### ðŸ“Š LatÃªncia\n\n"
        md_content += "| Batch | Mean (ms) | Std (ms) | P95 (ms) | P99 (ms) | Max (ms) | Per-Sample (Âµs) | Throughput (samples/s) |\n"
        md_content += "|-------|-----------|----------|----------|----------|----------|-----------------|------------------------|\n"
        for bs in sorted(deployment_metrics["latency"].keys()):
            lr = deployment_metrics["latency"][bs]
            md_content += f"| {bs:5d} | {lr['mean_ms']:9.3f} | {lr['std_ms']:8.3f} | {lr['p95_ms']:8.3f} | {lr['p99_ms']:8.3f} | {lr['max_ms']:8.3f} | {lr['per_sample_us']:15.2f} | {lr['throughput_samples_per_sec']:22.1f} |\n"
        
        md_content += f"\n**âœ“ Processing Time per Sample:** {deployment_metrics['processing_time_per_sample_us']:.2f} Âµs\n\n"
        
        # Latency Scaling
        if deployment_metrics.get("latency_scaling"):
            ls = deployment_metrics["latency_scaling"]
            md_content += f"**Latency Scaling (Batch 1â†’128):** {ls['interpretation']} (efficiency factor: {ls['batch_1_to_128_efficiency']:.2f})\n\n"
        
        # SeÃ§Ã£o de MemÃ³ria
        md_content += f"### ðŸ’¾ MemÃ³ria\n\n"
        md_content += "| MÃ©trica | Valor |\n"
        md_content += "|---------|-------|\n"
        if deployment_metrics.get("model_size_mb") is not None:
            md_content += f"| Model size (serialized) | {deployment_metrics['model_size_mb']:.2f} MB |\n"
        md_content += f"| Runtime memory (peak) | {deployment_metrics['runtime_memory_mb']:.2f} MB |\n"
        md_content += f"| Memory overhead | {deployment_metrics['memory_overhead_mb']:.2f} MB |\n"
        md_content += f"| **Total estimate** | **{deployment_metrics['total_memory_estimate_mb']:.2f} MB** |\n"
        md_content += f"| Memory feasibility | {deployment_metrics['memory_feasibility']} |\n\n"
        
        # SeÃ§Ã£o de Deployment/Real-time
        md_content += f"### âš¡ Deployment / Real-time\n\n"
        md_content += "| MÃ©trica | Valor |\n"
        md_content += "|---------|-------|\n"
        goose_status = "âœ“ COMPATIBLE" if deployment_metrics['goose_compatible'] else "âœ— NOT COMPATIBLE"
        goose_latency = deployment_metrics['latency'].get(1, {}).get('mean_ms', 0.0)
        md_content += f"| GOOSE deadline compatibility (3ms) | {goose_status} ({goose_latency:.3f}ms) |\n"
        md_content += f"| Real-time capability | {deployment_metrics['real_time_capability_msg_per_sec']:.1f} msg/s |\n\n"
        
        md_content += f"**ðŸ“Œ Notas sobre Deployment:**\n\n"
        md_content += f"- **Overhead in passive monitoring:** A latÃªncia adicional depende da integraÃ§Ã£o com o sistema de monitoramento\n"
        md_content += f"- **Embedded device performance:** Os valores apresentados sÃ£o estimativas; testes em hardware real sÃ£o recomendados\n"
        md_content += f"- **Efficiency trade-offs:** Considere tÃ©cnicas de quantizaÃ§Ã£o para reduzir model size e latÃªncia em ~2-4x\n\n"

    # ========================================
    # InterpretaÃ§Ã£o
    # ========================================

    md_content += f"---\n\n## ðŸ“ˆ InterpretaÃ§Ã£o\n\n"

    # Melhor classe (maior F1 no teste)
    best_class_idx = np.argmax([test_metrics['F1-score'][i] for i in range(len(class_names))])
    best_class = class_names[best_class_idx]
    best_f1 = test_metrics['F1-score'][best_class_idx]

    md_content += f"- **Melhor desempenho:** Classe `{best_class}` com F1-Score de **{best_f1:.4f}**\n"

    # Kappa interpretation
    if test_kappa > 0.8:
        kappa_interp = "ConcordÃ¢ncia **quase perfeita**"
    elif test_kappa > 0.6:
        kappa_interp = "ConcordÃ¢ncia **substancial**"
    elif test_kappa > 0.4:
        kappa_interp = "ConcordÃ¢ncia **moderada**"
    elif test_kappa > 0.2:
        kappa_interp = "ConcordÃ¢ncia **fraca**"
    else:
        kappa_interp = "ConcordÃ¢ncia **pobre**"

    md_content += f"- **Cohen's Kappa ({test_kappa:.4f}):** {kappa_interp}\n"

    md_content += f"\n---\n\n"
    md_content += f"*RelatÃ³rio gerado automaticamente pelo pipeline de treinamento {MODEL_TYPE.upper()}*\n"

    # Salva o arquivo Markdown
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)

    # ========================================
    # GERA RELATÃ“RIO EM LOG (texto simples)
    # ========================================

    log_content = f"""{'='*80}
RELATÃ“RIO DE MÃ‰TRICAS - {MODEL_TYPE.upper()}
{'='*80}

Dataset: {dataset_name}
Data/Hora: {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}

{'='*80}
VALIDAÃ‡ÃƒO CRUZADA (K-Fold) - MÃ©dia Â± IC 95%
{'='*80}

"""

    # MÃ©tricas CV em texto
    for i, cls in enumerate(class_names):
        log_content += f"\nClasse: {cls}\n"
        log_content += f"  F1-Score:  {cv_metrics['F1-score Mean'][i]:.4f} Â± {cv_metrics['F1-score CI'][i]:.4f}\n"
        log_content += f"  Precision: {cv_metrics['Precision Mean'][i]:.4f} Â± {cv_metrics['Precision CI'][i]:.4f}\n"
        log_content += f"  Recall:    {cv_metrics['Recall Mean'][i]:.4f} Â± {cv_metrics['Recall CI'][i]:.4f}\n"
        log_content += f"  Accuracy:  {cv_metrics['Accuracy Mean'][i]:.4f} Â± {cv_metrics['Accuracy CI'][i]:.4f}\n"

    log_content += f"\nCohen's Kappa (CV): {kappa_mean:.4f} Â± {kappa_ci:.4f}\n"

    # Inclui matriz de confusÃ£o agregada da CV, se disponÃ­vel
    if cv_total_cm is not None:
        log_content += f"\n{'-'*80}\n"
        log_content += "MATRIZ DE CONFUSÃƒO (CV - Agregada)\n"
        log_content += f"{'-'*80}\n\n"

        # Header
        log_content += "Predito â†’    "
        for cls in class_names:
            log_content += f"{cls:>12} "
        log_content += "\nReal â†“\n"

        for i, cls in enumerate(class_names):
            log_content += f"{cls:12} "
            for j in range(len(class_names)):
                log_content += f"{cv_total_cm[i][j]:>12} "
            log_content += "\n"

        log_content += f"\n{'='*80}\n"

    log_content += f"\n{'='*80}\n"
    log_content += f"TESTE FINAL (Hold-out)\n"
    log_content += f"{'='*80}\n"

    # MÃ©tricas teste em texto
    for i, cls in enumerate(class_names):
        log_content += f"\nClasse: {cls}\n"
        log_content += f"  F1-Score:  {test_metrics['F1-score'][i]:.4f}\n"
        log_content += f"  Precision: {test_metrics['Precision'][i]:.4f}\n"
        log_content += f"  Recall:    {test_metrics['Recall'][i]:.4f}\n"
        log_content += f"  Accuracy:  {test_metrics['Accuracy'][i]:.4f}\n"

    log_content += f"\nCohen's Kappa (Teste): {test_kappa:.4f}\n"

    # Matriz de confusÃ£o
    log_content += f"\n{'-'*80}\n"
    log_content += "MATRIZ DE CONFUSÃƒO (Teste Final)\n"
    log_content += f"{'-'*80}\n\n"

    # Header
    log_content += "Predito â†’    "
    for cls in class_names:
        log_content += f"{cls:>12} "
    log_content += "\nReal â†“\n"

    # Linhas
    for i, cls in enumerate(class_names):
        log_content += f"{cls:12} "
        for j in range(len(class_names)):
            log_content += f"{test_cm[i][j]:>12} "
        log_content += "\n"

    # ========================================
    # DESEMPENHO DE INFERÃŠNCIA (LOG)
    # ========================================
    if deployment_metrics is not None:
        log_content += f"\n{'='*80}\n"
        log_content += f"DESEMPENHO DE INFERÃŠNCIA\n"
        log_content += f"{'='*80}\n\n"
        
        log_content += "ðŸ“Š LATÃŠNCIA:\n"
        for bs in sorted(deployment_metrics["latency"].keys()):
            lr = deployment_metrics["latency"][bs]
            log_content += f"  Batch {bs:3d}: Mean={lr['mean_ms']:7.3f}ms | Std={lr['std_ms']:6.3f}ms | "
            log_content += f"P95={lr['p95_ms']:7.3f}ms | P99={lr['p99_ms']:7.3f}ms | Max={lr['max_ms']:7.3f}ms\n"
            log_content += f"             Per-sample={lr['per_sample_us']:8.2f}Âµs | Throughput={lr['throughput_samples_per_sec']:8.1f} samples/s\n"
        
        log_content += f"\nâœ“ Processing Time per Sample: {deployment_metrics['processing_time_per_sample_us']:.2f} Âµs\n"
        
        if deployment_metrics.get("latency_scaling"):
            ls = deployment_metrics["latency_scaling"]
            log_content += f"  Latency scaling (1â†’128): {ls['interpretation']} (factor: {ls['batch_1_to_128_efficiency']:.2f})\n"
        
        log_content += "\nðŸ’¾ MEMÃ“RIA:\n"
        if deployment_metrics.get("model_size_mb") is not None:
            log_content += f"  Model size (serialized):  {deployment_metrics['model_size_mb']:8.2f} MB\n"
        log_content += f"  Runtime memory (peak):    {deployment_metrics['runtime_memory_mb']:8.2f} MB\n"
        log_content += f"  Memory overhead:          {deployment_metrics['memory_overhead_mb']:8.2f} MB\n"
        log_content += f"  Total estimate:           {deployment_metrics['total_memory_estimate_mb']:8.2f} MB\n"
        log_content += f"  Memory feasibility:       {deployment_metrics['memory_feasibility']}\n"
        
        log_content += "\nâš¡ DEPLOYMENT / REAL-TIME:\n"
        goose_status = "âœ“ COMPATIBLE" if deployment_metrics['goose_compatible'] else "âœ— NOT COMPATIBLE"
        goose_latency = deployment_metrics['latency'].get(1, {}).get('mean_ms', 0.0)
        log_content += f"  GOOSE deadline (3ms):     {goose_status} ({goose_latency:.3f}ms)\n"
        log_content += f"  Real-time capability:     {deployment_metrics['real_time_capability_msg_per_sec']:.1f} msg/s\n"
        
        log_content += "\nðŸ“Œ NOTAS:\n"
        log_content += "  - Overhead in passive monitoring: LatÃªncia adicional depende de integraÃ§Ã£o com sistema\n"
        log_content += "  - Embedded device performance: Valores sÃ£o estimativas; teste em hardware real recomendado\n"
        log_content += "  - Efficiency trade-offs: Considere quantizaÃ§Ã£o para reduzir model size e latÃªncia\n"

    log_content += f"\n{'='*80}\n"
    log_content += f"RelatÃ³rio salvo em: {md_path}\n"
    log_content += f"{'='*80}\n"

    # Salva o arquivo Log
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(log_content)

    return md_path, log_path